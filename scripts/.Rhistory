state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "school.city"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.city <- GetCity("WA")
# get lats of schools
GetLat <- function(input.lat) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "float.lat")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "float.lat"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.lat <- GetLat("WA")
# get longs of schools
GetLong <- function(input.city) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "float.long")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "float.long"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.long <- GetLong("WA")
# get admission rates of schools
GetAdmissionsRate <- function(input.state) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "2015.admissions.admission_rate.overall")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "2015.admissions.admission_rate.overall"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.admissions <- GetAdmissionsRate("WA")
# get number of bachelors of library sciences at schools
GetBachLib <- function(input.state) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "academics.program.bachelors.library")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "academics.program.bachelors.library"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.bachelors <- GetBachLib("WA")
# get percent of first generation students
GetFirstGen <- function(input.state) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "student.share_firstgeneration")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "student.share_firstgeneration"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.firstgen <- GetFirstGen("WA")
library(jsonlite)
library(httr)
library(dplyr)
source("api_key.R")
#Jody's working directory
setwd("~/Downloads/INFO201/info201-ab5")
# the error that i get "No encoding supplied: defaulting to UTF-8."
# get school names
GetSchoolName <- function(input.school) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "school.name")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "school.name"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- flatten(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.names <- GetSchoolName("OR")
# get state
GetState <- function(input.state) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "school.state")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "school.state"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.state <- GetState("OR")
# get cities of schools
GetCity <- function(input.city) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "school.city")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "school.city"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.city <- GetCity("WA")
# get lats of schools
GetLat <- function(input.lat) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "float.lat")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "float.lat"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.lat <- GetLat("WA")
# get longs of schools
GetLong <- function(input.city) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "float.long")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "float.long"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.long <- GetLong("WA")
# get admission rates of schools
GetAdmissionsRate <- function(input.state) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "2015.admissions.admission_rate.overall")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "2015.admissions.admission_rate.overall"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.admissions <- GetAdmissionsRate("WA")
# get number of bachelors of library sciences at schools
GetBachLib <- function(input.state) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "academics.program.bachelors.library")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "academics.program.bachelors.library"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.bachelors <- GetBachLib("WA")
# get percent of first generation students
GetFirstGen <- function(input.state) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = api.key, fields = "student.share_firstgeneration")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
str(body.data)
names(body.data)
items <- body.data$items
is.data.frame(items)
state.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
query.params$fields <- "student.share_firstgeneration"
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- f(body.data$results)
state.data <- rbind(state.data, page.data) #merging the current state data with the current page data
}
return(state.data)
}
school.firstgen <- GetFirstGen("WA")
shiny::runApp('Desktop/Info201/info201-ab5')
library(shiny)
library(plotly)
library(dplyr)
library(RColorBrewer)
source("./scripts/financial.R")
source("./scripts/ethnicity.R")
# Define server logic required to draw a histogram
shinyServer(function(input, output) {
#Financial Data Table
output$finTable <- renderDataTable(GetFinData(input$year))
#Map of Schools
output$map <- renderPlotly ({
saved.data <- read.csv('./scripts/map.data.csv')
map.data <- saved.data %>%
filter(as.numeric(admissions) <= as.numeric(input$admissions))
g <- list(
scope = 'usa',
projection = list(type = 'albers usa'),
showland = TRUE,
landcolor = toRGB("gray95"),
subunitcolor = toRGB("gray25"),
countrycolor = toRGB("gray85"),
countrywidth = 0.5,
subunitwidth = 0.5
)
plot.interactive.map <- plot_geo(map.data, lat = ~lat, lon = ~long) %>%
add_markers(
text = ~paste(paste('School Name:', name), paste('City:', city), paste('Acceptance Rate:', admissions), paste('First Generation Student Percentage:', firstgen), sep = "<br />"),
color = ~admissions, symbol = I("square"), size = I(4), hoverinfo = "text"
) %>%
colorbar(title = "Acceptance Rate") %>%
layout(
title = 'Colleges Across the Country<br />(Hover For More Info)', geo = g
)
})
output$piechart <- renderPlotly({
pie.data <- GetRaceData(input$year) %>%
filter(school.name == input$school)
row.list <- unname(unlist(pie.data[1,]))
data.frame(t(row.list))
# Make chart
plot_ly(pie.data, labels = ~colnames(pie.data), values = ~row.list, type = 'pie',
marker = list(colors = colorRampPalette(brewer.pal(12, "Set3"))(100))) %>%
layout(title = "Percentages by Race",  showlegend = F,
xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))
})
})
runApp('Desktop/Info201/info201-ab5')
runApp('Desktop/Info201/info201-ab5')
runApp('Desktop/Info201/info201-ab5')
runApp('Desktop/Info201/info201-ab5')
setwd("~/Desktop/Info201/info201-ab5")
library(jsonlite)
library(httr)
library(dplyr)
# https://api.data.gov/ed/collegescorecard/v1/schools?api_key=fDjxnzknPKeKvMcOlzCAb6aK3IhayEClNrqG4zxF&fields=school.name,school.state,school.city,location.lat,location.lon,2015.admissions.admission_rate.overall,2015.academics.program.bachelors.library,2015.student.share_firstgeneration&sort=school.name
jody.key <- "mlKZJBln7XFc6e3ftRm1TCqVwwRlxfWYGODVpqhe"
GetData <- function(input.year) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = jody.key, fields = "school.name,school.state,school.city,location.lat,location.lon")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
school.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
all.data <- paste0("school.name,school.state,school.city,location.lat,location.lon,",
input.year, ".admissions.admission_rate.overall,",
input.year, ".academics.program.bachelors.library,",
input.year, ".student.share_firstgeneration")
query.params$fields <- paste(all.data)
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- flatten(body.data$results)
school.data <- rbind(school.data, page.data) #merging the current state data with the current page data
}
return(school.data)
}
school.info <- GetData("2015")
library(jsonlite)
library(httr)
library(dplyr)
# https://api.data.gov/ed/collegescorecard/v1/schools?api_key=fDjxnzknPKeKvMcOlzCAb6aK3IhayEClNrqG4zxF&fields=school.name,school.state,school.city,location.lat,location.lon,2015.admissions.admission_rate.overall,2015.academics.program.bachelors.library,2015.student.share_firstgeneration&sort=school.name
jody.key <- "mlKZJBln7XFc6e3ftRm1TCqVwwRlxfWYGODVpqhe"
GetData <- function(input.year) {
base.uri <- 'https://api.data.gov/ed/collegescorecard/v1/schools/'
query.params <- list(api_key = jody.key, fields = "school.name,school.state,school.city,location.lat,location.lon")
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
school.data <- data.frame()
#get total number of pages by dividing total data and num of data per page
all.pages <- trunc(body.data$metadata$total / body.data$metadata$per_page)
#for loop to each page and add that page's data into state.data
for(p in 1:all.pages) {
all.data <- paste0("school.name,school.state,school.city,location.lat,location.lon,",
input.year, ".admissions.admission_rate.overall,",
input.year, ".academics.program.bachelors.library,",
input.year, ".student.share_firstgeneration")
query.params$fields <- paste(all.data)
query.params$page <- p
response <- GET(base.uri, query = query.params)
content <- content(response, "text")
body.data <- fromJSON(content) #extract and parse
page.data <- flatten(body.data$results)
school.data <- rbind(school.data, page.data) #merging the current state data with the current page data
}
return(school.data)
}
school.info <- GetData("2015")
school.info <- school.info %>%
mutate(`2015.student.share_firstgeneration` = `2015.student.share_firstgeneration` * 100,
`2015.admissions.admission_rate.overall` = `2015.admissions.admission_rate.overall` * 100)
colnames(school.info) <- c("lat", "long", "name", "library", "state", "city", "firstgen", "admissions")
write.csv(school.info, file = 'map.data.csv', row.names = FALSE)
school.info <- GetData("2015")
# https://api.data.gov/ed/collegescorecard/v1/schools?api_key=fDjxnzknPKeKvMcOlzCAb6aK3IhayEClNrqG4zxF&fields=school.name,school.state,school.city,location.lat,location.lon,2015.admissions.admission_rate.overall,2015.academics.program.bachelors.library,2015.student.share_firstgeneration&sort=school.name
jody.key <- "8iWSNJe4uL6mxfqd55ofg3e9PdFABs5qG8kC1PgZ"
school.info <- GetData("2015")
View(school.info)
school.info <- school.info %>%
mutate(`2015.student.share_firstgeneration` = `2015.student.share_firstgeneration` * 100,
`2015.admissions.admission_rate.overall` = `2015.admissions.admission_rate.overall` * 100)
colnames(school.info) <- c("lat", "long", "name", "library", "state", "city", "firstgen", "admissions")
write.csv(school.info, file = 'map.data.csv', row.names = FALSE)
setwd("~/Desktop/Info201/info201-ab5/scripts")
write.csv(school.info, file = 'map.data.csv', row.names = FALSE)
saved.data <- read.csv('map.data.csv')
map.data <- saved.data %>%
filter(as.numeric(admissions) <= as.numeric(input$admissions))
runApp('~/Desktop/Info201/info201-ab5')
View(fin.data)
